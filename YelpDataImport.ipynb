{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary libraries.\n",
    "from py2neo import Graph\n",
    "import reverse_geocoder as rg\n",
    "import json, csv, uuid, os, regex as re, subprocess, datetime, numpy as np, pandas as pd\n",
    "from typing import List, Tuple, Set\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "from neo4j.exceptions import ServiceUnavailable, TransientError\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Const.\n",
    "IN_CATEGORY = \"IN_CATEGORY\"\n",
    "IN_CITY = \"IN_CITY\"\n",
    "IN_AREA = \"IN_AREA\"\n",
    "IN_COUNTRY = \"IN_COUNTRY\"\n",
    "FRIENDS = \"FRIENDS\"\n",
    "REVIEWS = \"REVIEWS\"\n",
    "WROTE = \"WROTE\"\n",
    "BUSINESS_NODE = \"Business\"\n",
    "USER_NODE = \"User\"\n",
    "REVIEW_NODE = \"Review\"\n",
    "CATEGORY_NODE = \"Category\"\n",
    "CITY_NODE = \"City\"\n",
    "AREA_NODE = \"Area\"\n",
    "COUNTRY_NODE = \"Country\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important locations.\n",
    "data_folder = r\"./data\"\n",
    "business_json_file = data_folder + \"/yelp_academic_dataset_business.json\"\n",
    "review_json_file = data_folder + \"/yelp_academic_dataset_review.json\"\n",
    "user_json_file = data_folder + \"/yelp_academic_dataset_user.json\"\n",
    "fixed_business_json_file = data_folder + \"/fixed_yelp_academic_dataset_business.json\"\n",
    "fixed_review_json_file = data_folder + \"/fixed_yelp_academic_dataset_review.json\"\n",
    "fixed_user_json_file = data_folder + \"/fixed_yelp_academic_dataset_user.json\"\n",
    "list_raw_files = [business_json_file, review_json_file, user_json_file]\n",
    "list_fixed_data = [fixed_business_json_file, fixed_review_json_file, fixed_user_json_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Csv files for Import Tool.\n",
    "business_nodes_csv_file = \"business_nodes.csv\"\n",
    "category_nodes_csv_file = \"category_nodes.csv\"\n",
    "city_nodes_csv_file = \"city_nodes.csv\"\n",
    "area_nodes_csv_file = \"area_nodes.csv\"\n",
    "country_nodes_csv_file = \"country_nodes.csv\"\n",
    "user_nodes_csv_file = \"user_nodes.csv\"\n",
    "review_nodes_csv_file = \"review_nodes.csv\"\n",
    "relationship_csv_file = \"relationships.csv\"\n",
    "nodes_files = [business_nodes_csv_file, category_nodes_csv_file, city_nodes_csv_file, area_nodes_csv_file, country_nodes_csv_file, user_nodes_csv_file, review_nodes_csv_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to Neo4j Db.\n",
    "graph_name = \"neo4j\" # Default for 4.0: neo4j; default for 3.5: graph.db\n",
    "SERVER_ADDRESS = \"bolt://localhost:7687\"\n",
    "SERVER_AUTH = (\"neo4j\", \"12345\")\n",
    "graph = Graph(SERVER_ADDRESS, auth=SERVER_AUTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_home = graph.service.config[\"dbms.directories.neo4j_home\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_files(files: List[str]):\n",
    "    for one_file in files:\n",
    "        one_path = Path(one_file)\n",
    "        if one_path.is_file():\n",
    "            one_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all(list(map(lambda x: Path(x).is_file(), list_raw_files))):\n",
    "    raise Exception(f'Not all Yelp raw files are available. Need: {list_raw_files}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Problems:**  \n",
    "1. Ids are unique only in the specific domain. E.g. `business_id` in Business `oiAlXZPIFm2nBCt0DHLu_Q` is identical to that `user_id` in User.\n",
    "When using Neo4j's Import Tool, the relationship csv defines \"from-node\" and \"to-node\", identified \n",
    "by unique ids. Thus, the entities' ids, regardless of Business or User or Review must be unique.\n",
    "2. Users have friends, but not all the friends exist. E.g. Unknown friend: `oeMvJh94PiGQnx_6GlndPQ` for User `ntlvfPzc8eglqvk92iDIAw`. Those unknown friends must be removed.\n",
    "3. Some Business contain duplicate categories, e.g. Business with id `HEGy1__jKyMhkhXRW3O1ZQ` has duplicated `Gas Stations`. These must be deduplicated.\n",
    "4. In Users, `friends` is supposed to be an array with string elements. But in reality, it is a string in which friend_ids are concatenated with commas. Same problem with `categories` field in Business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix data problems.\n",
    "def remove_unknown_friends(raw_path: str, output_path: str) -> None:\n",
    "    user_ids = set()\n",
    "    with open(raw_path, \"r\", encoding=\"utf-8\") as rf:\n",
    "        for line in rf:\n",
    "            if len(line.strip()) > 0:\n",
    "                json_node = json.loads(line)\n",
    "                user_ids.add(json_node[\"user_id\"])\n",
    "            \n",
    "    with open(raw_path, \"r\", encoding=\"utf-8\") as rf, open(output_path, mode=\"w\", encoding=\"utf-8\") as of:\n",
    "        for line in rf:\n",
    "            if len(line.strip()) > 0:\n",
    "                json_node = json.loads(line)\n",
    "                friends_str = json_node[\"friends\"]\n",
    "                if friends_str and len(friends_str.strip()) > 0:\n",
    "                    friends_arr = re.split(r\"\\s*,\\s*\", friends_str.strip())\n",
    "                    friends_exist_arr = list(filter(lambda x: x in user_ids, friends_arr))\n",
    "                    json_node[\"friends\"] = ', '.join(friends_exist_arr)\n",
    "                    of.write(f'{json.dumps(json_node)}\\n')\n",
    "\n",
    "                    \n",
    "def make_ids_unique(input_path: str, output_path: str, func_to_modify) -> None:\n",
    "    temp_output_file = str(uuid.uuid4())\n",
    "    with open(input_path, mode=\"r\", encoding=\"utf-8\") as in_f, open(temp_output_file, mode=\"w\", encoding=\"utf-8\") as ou_f:\n",
    "        for line in in_f:\n",
    "            if len(line.strip())>0:\n",
    "                json_node = json.loads(line)\n",
    "                json_node = func_to_modify(json_node)\n",
    "                ou_f.write(f'{json.dumps(json_node)}\\n')\n",
    "    # Rename the file.\n",
    "    os.replace(temp_output_file, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the fixed files are already available.\n",
    "if not all(list(map(lambda x: Path(x).is_file(), list_fixed_data))):\n",
    "    # Remove everything first.\n",
    "    delete_files(list_fixed_data)\n",
    "    # Re-create the fixed files.\n",
    "    def fix_user(json_node):\n",
    "        json_node[\"user_id\"] = \"u-\" + json_node[\"user_id\"]\n",
    "        friends_str = json_node[\"friends\"]\n",
    "        if friends_str and len(friends_str.strip()) > 0:\n",
    "            friends_arr = re.split(r\"\\s*,\\s*\", friends_str.strip())\n",
    "            friends_arr = list(map(lambda x: \"u-\" + x, friends_arr))\n",
    "            json_node[\"friends\"] = ', '.join(friends_arr)\n",
    "        return json_node\n",
    "    def fix_business(json_node):\n",
    "        json_node[\"business_id\"] = \"b-\" + json_node[\"business_id\"]\n",
    "        if json_node[\"categories\"] and len(json_node[\"categories\"].strip())>0:\n",
    "            categories_arr = re.split(\"\\s*,\\s*\", json_node[\"categories\"].strip())\n",
    "            categories_set = set(categories_arr)\n",
    "            if len(categories_set) < len(categories_arr):\n",
    "                json_node[\"categories\"] = ', '.join(categories_set)\n",
    "        return json_node\n",
    "    def fix_review(json_node):\n",
    "        json_node[\"review_id\"] = \"r-\" + json_node[\"review_id\"]\n",
    "        json_node[\"user_id\"] = \"u-\" + json_node[\"user_id\"]\n",
    "        json_node[\"business_id\"] = \"b-\" + json_node[\"business_id\"]\n",
    "        return json_node\n",
    "    \n",
    "    # Start fixing data problems.\n",
    "    remove_unknown_friends(user_json_file, fixed_user_json_file)\n",
    "    make_ids_unique(fixed_user_json_file, fixed_user_json_file, fix_user)\n",
    "    make_ids_unique(business_json_file, fixed_business_json_file, fix_business)\n",
    "    make_ids_unique(review_json_file, fixed_review_json_file, fix_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating inputs for Import Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading formatted geocoded file...\n"
     ]
    }
   ],
   "source": [
    "if not all(list(map(lambda x: Path(x).is_file(), nodes_files))) or not Path(relationship_csv_file).is_file():\n",
    "    # Delete everything.\n",
    "    delete_files(nodes_files)\n",
    "    delete_files([relationship_csv_file])\n",
    "    # Using sets to ensures uniqueness.\n",
    "    business_lat_lon = {}\n",
    "    area_nodes: Set[Tuple[str, str]] = set()\n",
    "    city_nodes: Set[Tuple[str, str]]= set()\n",
    "    country_nodes: Set[str] = set()\n",
    "    categories_nodes: Set[str] = set()\n",
    "    in_area_relationships: Set[Tuple[str, str, str]] = set()\n",
    "    in_country_relationships: Set[Tuple[str, str, str]] = set()\n",
    "    # Relationship writer.\n",
    "    relationship_csv = open(relationship_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\\n\")\n",
    "    relationship_fieldnames = [\":START_ID\", \":END_ID\", \":TYPE\"]\n",
    "    relationship_writer = csv.DictWriter(relationship_csv, fieldnames=relationship_fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    relationship_writer.writeheader()\n",
    "    \n",
    "    # Func to write Business Nodes to file.\n",
    "    def write_business_nodes_to_file():\n",
    "        with open(fixed_business_json_file, \"r\", encoding=\"utf-8\") as bjf, open(business_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\\n\") as business_nodes_csv:\n",
    "            fieldnames = [\"business_id:ID\", \"name\", \"address\", \":LABEL\"]\n",
    "            writer = csv.DictWriter(business_nodes_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writeheader()\n",
    "            # File too large, read line by line.\n",
    "            for line in bjf:\n",
    "                line = line.strip()\n",
    "                if len(line)>0:\n",
    "                    json_node = json.loads(line)\n",
    "                    writer.writerow({k:v for k, v in zip(fieldnames, [json_node[\"business_id\"], json_node[\"name\"], json_node[\"address\"], BUSINESS_NODE])})\n",
    "                    business_lat_lon[json_node[\"business_id\"]] = (json_node[\"latitude\"], json_node[\"longitude\"])\n",
    "                    if json_node[\"categories\"] and len(json_node[\"categories\"].strip())>0: # can be None (e.g. for Business Id: 2W1tLg8ybRUEKMPoAPHTsQ)\n",
    "                        cur_categories = list(filter(lambda x: len(x)>0, map(lambda x: x.strip(), re.split(\"\\s*,\\s*\", json_node[\"categories\"].strip()))))\n",
    "                        categories_nodes.update(cur_categories)\n",
    "                        for category in cur_categories:\n",
    "                            relationship_writer.writerow({k:v for k, v in zip(relationship_fieldnames, [json_node[\"business_id\"], category, IN_CATEGORY])})\n",
    "    \n",
    "    # Func to write Category Nodes to file.\n",
    "    def write_category_nodes_to_file():\n",
    "        with open(category_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\\n\") as category_nodes_csv:\n",
    "            fieldnames = [\"category_id:ID\", \":LABEL\"]\n",
    "            writer = csv.DictWriter(category_nodes_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writeheader()\n",
    "            for category_id in categories_nodes:\n",
    "                writer.writerow({k:v for k, v in zip(fieldnames, [category_id, CATEGORY_NODE])})\n",
    "                \n",
    "    # Func to make City, Area, Country Nodes.\n",
    "    def make_city_area_country_nodes():\n",
    "        lat_lons = list(business_lat_lon.values())\n",
    "        city_state_countries = rg.search(lat_lons)\n",
    "        # Process City, Area, Country Nodes; save IN_CITY relationship to file.\n",
    "        for (business_id, city_state_country) in list(zip(list(business_lat_lon.keys()), city_state_countries)):\n",
    "            city, state, country = city_state_country[\"name\"], city_state_country[\"admin1\"], city_state_country[\"cc\"]\n",
    "            unique_state = f'{state}-{country}'\n",
    "            unique_city = f'{city}-{state}-{country}'\n",
    "            country_nodes.add(country)\n",
    "            area_nodes.add((unique_state, state))\n",
    "            city_nodes.add((unique_city, city))\n",
    "            # Create corresponding relationships.\n",
    "            relationship_writer.writerow({k:v for k, v in zip(relationship_fieldnames, [business_id, unique_city, IN_CITY])})\n",
    "            in_area_relationships.add((unique_city, unique_state, IN_AREA))\n",
    "            in_country_relationships.add((unique_state, country, IN_COUNTRY))\n",
    "    \n",
    "    # Func to write City, Area, Country Nodes to file.\n",
    "    def write_city_area_country_nodes_to_file():\n",
    "        with open(city_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\\n\") as city_nodes_csv:\n",
    "            fieldnames = [\"city_id:ID\", \"name\", \":LABEL\"]\n",
    "            writer = csv.DictWriter(city_nodes_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writeheader()\n",
    "            for (city_id, city_name) in city_nodes:\n",
    "                writer.writerow({k:v for k, v in zip(fieldnames, [city_id, city_name, CITY_NODE])})\n",
    "\n",
    "        with open(area_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\\n\") as area_nodes_csv:\n",
    "            fieldnames = [\"area_id:ID\", \"name\", \":LABEL\"]\n",
    "            writer = csv.DictWriter(area_nodes_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writeheader()\n",
    "            for (area_id, area_name) in area_nodes:\n",
    "                writer.writerow({k:v for k, v in zip(fieldnames, [area_id, area_name, AREA_NODE])})\n",
    "\n",
    "        with open(country_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\\n\") as country_nodes_csv:\n",
    "            fieldnames = [\"country_id:ID\", \":LABEL\"]\n",
    "            writer = csv.DictWriter(country_nodes_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writeheader()\n",
    "            for country_id in country_nodes:\n",
    "                writer.writerow({k:v for k, v in zip(fieldnames, [country_id, COUNTRY_NODE])})\n",
    "                \n",
    "        for (u1, u2, rel_type) in in_area_relationships:\n",
    "            relationship_writer.writerow({k:v for k, v in zip(relationship_fieldnames, [u1, u2, rel_type])})\n",
    "\n",
    "        for (u1, u2, rel_type) in in_country_relationships:\n",
    "            relationship_writer.writerow({k:v for k, v in zip(relationship_fieldnames, [u1, u2, rel_type])})\n",
    "    \n",
    "    # Func to write User nodes to file.\n",
    "    def write_user_nodes_to_file():\n",
    "        friend_relationships: Set[Tuple[str, str, str]] = set()\n",
    "        with open(fixed_user_json_file, \"r\", encoding=\"utf-8\") as ujf, open(user_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\\n\") as user_nodes_csv:\n",
    "            fieldnames = [\"user_id:ID\", \"name\", \"yelping_since\", \":LABEL\"]\n",
    "            writer = csv.DictWriter(user_nodes_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writeheader()\n",
    "            # File too large, read line by line.\n",
    "            for line in ujf:\n",
    "                line = line.strip()\n",
    "                if len(line)>0:\n",
    "                    json_node = json.loads(line)\n",
    "                    writer.writerow({k:v for k, v in zip(fieldnames, [json_node[\"user_id\"], json_node[\"name\"], json_node[\"yelping_since\"], USER_NODE])})\n",
    "                    if json_node[\"friends\"] and len(json_node[\"friends\"].strip()) > 0:\n",
    "                        friends_arr = re.split(\"\\s*,\\s*\", json_node[\"friends\"].strip())\n",
    "                        for friend_id in friends_arr:\n",
    "                            # Prevent duplicate friend relationship later!\n",
    "                            f1 = min(json_node[\"user_id\"], friend_id)\n",
    "                            f2 = max(json_node[\"user_id\"], friend_id)\n",
    "                            friend_relationships.add((f1, f2, FRIENDS))\n",
    "        \n",
    "        for (f1, f2, rel_type) in friend_relationships:\n",
    "            relationship_writer.writerow({k:v for k, v in zip(relationship_fieldnames, [f1, f2, rel_type])})\n",
    "    \n",
    "    # Func to write Review nodes to file.\n",
    "    def write_review_nodes_to_file():\n",
    "        with open(fixed_review_json_file, \"r\", encoding=\"utf-8\") as rjf, open(review_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\\n\") as review_nodes_csv:\n",
    "            fieldnames = [\"review_id:ID\", \"stars\", \"date\", \"text\", \":LABEL\"]\n",
    "            writer = csv.DictWriter(review_nodes_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writeheader()\n",
    "            # File too large, read line by line.\n",
    "            for line in rjf:\n",
    "                line = line.strip()\n",
    "                if len(line)>0:\n",
    "                    json_node = json.loads(line)\n",
    "                    writer.writerow({k:v for k, v in zip(fieldnames, [json_node[\"review_id\"], json_node[\"stars\"], json_node[\"date\"], json_node[\"text\"], REVIEW_NODE])})\n",
    "                    relationship_writer.writerow({k:v for k, v in zip(relationship_fieldnames, [json_node[\"user_id\"], json_node[\"review_id\"], WROTE])})\n",
    "                    relationship_writer.writerow({k:v for k, v in zip(relationship_fieldnames, [json_node[\"review_id\"], json_node[\"business_id\"], REVIEWS])})\n",
    "    \n",
    "    # Start re-creating CSV files for Import Tool.\n",
    "    write_business_nodes_to_file()\n",
    "    write_category_nodes_to_file()\n",
    "    del categories_nodes\n",
    "    make_city_area_country_nodes()\n",
    "    write_city_area_country_nodes_to_file()\n",
    "    del business_lat_lon, city_nodes, area_nodes, country_nodes, in_area_relationships, in_country_relationships\n",
    "    write_user_nodes_to_file()\n",
    "    write_review_nodes_to_file()\n",
    "    \n",
    "    relationship_csv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Data Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_nodes = 0\n",
    "\n",
    "# Func to if nodes are unique and relationships are also unique.\n",
    "def check_nodes_relationships_csv_files_integrity():\n",
    "    global num_of_nodes\n",
    "    # Nodes files.\n",
    "    for one_node_file in nodes_files:\n",
    "        temp_df = pd.read_csv(one_node_file, header = \"infer\", sep = \",\", encoding = \"utf-8\")\n",
    "        if len(temp_df.iloc[:, 0]) != len(np.unique(temp_df.iloc[:, 0].values)):\n",
    "            raise Exception(f'Nodes in [{one_node_file}]: not unique')\n",
    "        num_of_nodes += len(temp_df.iloc[:, 0])\n",
    "    # Relationship file.\n",
    "    temp_df = pd.read_csv(relationship_csv_file, header = \"infer\", sep = \",\", encoding = \"utf-8\")\n",
    "    rels = temp_df.iloc[:, 0] + temp_df.iloc[:, 1]\n",
    "    if len(rels) != len(np.unique(rels.values)):\n",
    "        raise Exception(f'Relationships in [{relationship_csv_file}]: not unique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_nodes_relationships_csv_files_integrity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing to Neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reseting the database and importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quickest/cleanest/safest way is:  \n",
    "1. Stop the Neo4j's Database Service.  \n",
    "2. Remove the neo4j's Database's Data folder.  \n",
    "3. Execute the Import Tool.  \n",
    "4. Start the Neo4j's Database Service.  \n",
    "\n",
    "**Important Note**: Users must have privileges to start/stop Windows services. If needing elevating, *a Windows popup will\n",
    "appear, asking for the permission*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Func to control Neo4j's Database Service.\n",
    "def command_neo4j_database_service(cmd: str):\n",
    "    neo4j_cmd = neo4j_home + r\"\\bin\\neo4j.bat\"\n",
    "    if cmd in [\"stop\", \"start\"]:\n",
    "        cmd_res = subprocess.run([neo4j_cmd, cmd], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        cmd_stdout = str(cmd_res.stdout, \"utf-8\")\n",
    "        if cmd == \"stop\" and \"stopped\" not in cmd_stdout:\n",
    "            raise Exception(f\"Can't stop Neo4j's Database Service [{cmd_res.stderr}]\")\n",
    "        elif cmd == \"start\" and \"started\" not in cmd_stdout:\n",
    "            raise Exception(f\"Can't start Neo4j's Database Service [{cmd_res.stderr}]\")\n",
    "    else:\n",
    "        raise Exception(f'Unknown command for Neo4j\\'s Database Service: [{cmd}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Func to remove the Database's Data folder.\n",
    "def reset_neo4j_database():\n",
    "    neo4j_graph_folder = rf\"{neo4j_home}\\data\\databases\\{graph_name}\"\n",
    "    neo4j_trans_folder = rf\"{neo4j_home}\\data\\transactions\\{graph_name}\"\n",
    "    if not Path(neo4j_graph_folder).is_dir():\n",
    "        raise Exception(\"Can't find Neo4j's Database's Data folder\")\n",
    "    shutil.rmtree(neo4j_graph_folder)\n",
    "    shutil.rmtree(neo4j_trans_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Func to import data into Neo4j's Database.\n",
    "def import_data():\n",
    "    import_tool_cmd = neo4j_home + r\"\\bin\\neo4j-admin.bat\"\n",
    "    arguments = [\"import\", \"--multiline-fields=true\"] + list(map(lambda x: f'--nodes={x}', nodes_files)) + [f\"--relationships={relationship_csv_file}\"]\n",
    "    # Execute the Import Tool (This tool will re-create a fresh \"neo4j\" folder if needed).\n",
    "    cmd_res = subprocess.run([import_tool_cmd] + arguments, cwd=os.getcwd(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    if \"IMPORT DONE\" not in str(cmd_res.stdout, \"utf-8\"):\n",
    "        raise Exception(f\"Can't execute Import Tool successfully [{cmd_res.stderr}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Neo4j's Database Service is already containing the desired data.\n",
    "if not(len(graph.nodes) == num_of_nodes and num_of_nodes > 0):\n",
    "    # Start resetting the database, then importing the data (nodes & relationships).\n",
    "    command_neo4j_database_service(\"stop\")\n",
    "    reset_neo4j_database()\n",
    "    import_data()\n",
    "    command_neo4j_database_service(\"start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if the importing is successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_graph_ready = False\n",
    "\n",
    "# Func to check if the importing process was done successfully.\n",
    "def check_if_importing_is_successful():\n",
    "    global graph\n",
    "    global yelp_graph_ready\n",
    "    num_tries = 30\n",
    "    for one_try in range(num_tries):\n",
    "        try:\n",
    "            graph = Graph(SERVER_ADDRESS, auth=SERVER_AUTH)\n",
    "            cur_num_nodes = len(graph.nodes)\n",
    "            if cur_num_nodes == 0:\n",
    "                raise Exception(f\"There is no node in the Database\")\n",
    "            if cur_num_nodes != num_of_nodes:\n",
    "                raise Exception(f'Expected: [{num_of_nodes}] nodes, but found: [{cur_num_nodes}]')\n",
    "            yelp_graph_ready = True\n",
    "        except (ConnectionRefusedError, ServiceUnavailable, TransientError, ConnectionAbortedError) as e:\n",
    "            print(f\"{e}. Try again...\")\n",
    "            sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 10053] An established connection was aborted by the software in your host machine. Try again...\n",
      "[WinError 10061] No connection could be made because the target machine actively refused it. Try again...\n",
      "[WinError 10061] No connection could be made because the target machine actively refused it. Try again...\n",
      "[WinError 10061] No connection could be made because the target machine actively refused it. Try again...\n",
      "[WinError 10061] No connection could be made because the target machine actively refused it. Try again...\n",
      "[WinError 10061] No connection could be made because the target machine actively refused it. Try again...\n",
      "[WinError 10061] No connection could be made because the target machine actively refused it. Try again...\n",
      "[WinError 10061] No connection could be made because the target machine actively refused it. Try again...\n",
      "[WinError 10061] No connection could be made because the target machine actively refused it. Try again...\n",
      "[WinError 10061] No connection could be made because the target machine actively refused it. Try again...\n",
      "[WinError 10061] No connection could be made because the target machine actively refused it. Try again...\n",
      "[WinError 10061] No connection could be made because the target machine actively refused it. Try again...\n",
      "[WinError 10061] No connection could be made because the target machine actively refused it. Try again...\n",
      "[WinError 10061] No connection could be made because the target machine actively refused it. Try again...\n",
      "[WinError 10061] No connection could be made because the target machine actively refused it. Try again...\n",
      "[WinError 10061] No connection could be made because the target machine actively refused it. Try again...\n",
      "[WinError 10061] No connection could be made because the target machine actively refused it. Try again...\n"
     ]
    }
   ],
   "source": [
    "check_if_importing_is_successful()\n",
    "if not yelp_graph_ready:\n",
    "    raise Exception(\"Tried waiting for Neo4j Database Service, but still not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Neo4j's Browser, one can execute cypher `CALL db.schema.visualization()` to get the Schema of the Graph. It should look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Expected Schema](expected_schema.jpg \"Expected Schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the raw data from [Yelp Dataset](https://www.yelp.com/dataset), we intentionally ignore \"Tip\" and \"Checking\" files. We take into account \"Business\", \"User\" and \"Review\" files. However, those files are **not** free from errors (see Data Problems above). Thus, we fix them and save into `fixed_....json` files.  \n",
    "\n",
    "Because the data files are heavy, e.g. \"Review\" (6GB), this Python script is prioritized in keeping memory footprint as little as possible, at the expense of running speed.  \n",
    "\n",
    "The quickest way to import a large amount of nodes and relationships into Neo4j is through Import Tool. It's implied that the *user should stay on the machine where Neo4j's Database Service is installed, and has sufficient privileges to start/stop Neo4j's Database Service*. Note: With Neo4j Desktop, to install a Database as a Service, we first create a Database, find out its location on disk, stop it (if it is running), then execute `bin\\neo4j.bat install-service` to install the Database as a Service, and `bin\\neo4j.bat start` to start it.  \n",
    "\n",
    "The `fixed_....json` files above are transformed into a list of CSV files with the formats expected by the [Import Tool](https://neo4j.com/docs/operations-manual/current/tutorial/import-tool/). Before continuing, we do some Data Integrity checking.  \n",
    "\n",
    "Next, we start importing the CSV files into Neo4j's Database Service *appropriately*. Finally, we check that the importing process is done successfully by counting the number of nodes.  \n",
    "\n",
    "**Tested environment**:\n",
    "+ Windows 8.1 x64  \n",
    "+ Anaconda3-2020.02 x64 (Python v3.7.6)  \n",
    "+ Neo4j **v4.0.4** (JDK v11.0.7 LTS x64), with `dbms.memory.heap.max_size` 4GB  \n",
    "+ Yelp Dataset (03.05.2020). MD5: `7610af013edf610706021697190dab15`  \n",
    "+ Neo4j Driver: Py2neo **v5.0b1**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neo4j",
   "language": "python",
   "name": "neo4j"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
